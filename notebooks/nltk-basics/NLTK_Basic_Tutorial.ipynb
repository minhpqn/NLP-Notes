{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Basic Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial instruct you how to perform basic text processing with [nltk](http://www.nltk.org) - a suite of libraries and programs for Natural Language Processing for English.\n",
    "\n",
    "## Download and Install nltk\n",
    "\n",
    "You can install nltk by using ```pip``` (See [http://www.nltk.org/install.html](http://www.nltk.org/install.html)). NLTK is included in [Anaconda](https://www.continuum.io/downloads). I recommend you to use Anaconda to ease python package management tasks.\n",
    "\n",
    "After downloading nltk, you should install [nltk.data](http://www.nltk.org/data.html).\n",
    "\n",
    "## Basic Text Processing Tasks\n",
    "\n",
    "Natural Language Content Analysis is a fundamental step in every text mining project. Depending on the text mining task, you may want to generate representations of text data in different levels. For instance, sentences in a text data may be simply represented as a **bag of words** or may be annotated with semantic word classes.\n",
    "\n",
    "In this tutorial, we will learn how to use nltk to perform:\n",
    "\n",
    "- Sentence segmentation\n",
    "- Word Tokenization\n",
    "- Word Lemmatization\n",
    "- Word Stemming\n",
    "- Filtering stop words.\n",
    "- Part-of-speech tagging\n",
    "- Extracting information from text\n",
    "  * Chunking\n",
    "  * Named Entity Recognition\n",
    "  * Relation Extraction\n",
    "- Analyzing structures of sentences\n",
    "  * Phrasal structure analysis\n",
    "  * Dependency parsing\n",
    "\n",
    "### Sentence segmentation\n",
    "\n",
    "In many cases, we want to split a document or paragraph into a list of sentences. For instance, we want to identify sentiment of a sentence in the sentiment analysis task, or we may want to analyze structures of sentences.\n",
    "\n",
    "To illustrate how to do it with ```nltk```, we first create a paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para = \"Hello World. It's good to see you. Thanks for buying this book.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to split ```para``` into sentences. We will use module ```nltk.tokenize``` to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a list of sentences for further processing.\n",
    "\n",
    "The instance used in ```sent_tokenize()``` is actually loaded on demand from a pickle file. So if you're going to be tokenizing a lot of sentences, it's more efficient to load the ```PunktSentenceTokenizer``` once, and call its ```tokenize()``` method instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello World.', \"It's good to see you.\", 'Thanks for buying this book.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "tokenizer.tokenize(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing a sentence into words\n",
    "\n",
    "In this section, we will tokenize a sentence into words. We can do the task with the basic word tokenization by using the function ```word_tokenization```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'history', 'of', 'NLP', 'generally', 'starts', 'in', 'the', '1950s', ',', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sent = 'The history of NLP generally starts in the 1950s, although work can be found from earlier periods.'\n",
    "print( word_tokenize(sent) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a list of tokens as above. \n",
    "\n",
    "```word_tokenize()``` is a wrapper function that calls ```tokenize()``` on an instance of the ```TreebankWordTokenizer```. It's equivalent to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'history', 'of', 'NLP', 'generally', 'starts', 'in', 'the', '1950s', ',', 'although', 'work', 'can', 'be', 'found', 'from', 'earlier', 'periods', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```TreebankWordTokenizer``` use conventions in Penn Treebank corpus. We can have many alternatives for word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ca', \"n't\", 'swim', '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I can't swim.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives of word tokenization\n",
    "\n",
    "#### WordPunctTokenizer\n",
    "\n",
    "```WordPunctTokenizer``` splits all punctuations into separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'can', \"'\", 't', 'swim', '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\"I can't swim.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexpTokenizer\n",
    "\n",
    "We can use regular expression to tokenize words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"can't\", 'swim']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"I can't swim.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can do in a simpler way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"can't\", 'swim']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "regexp_tokenize(\"I can't swim.\", \"[\\w']+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use simple whitespaces as delimiters for word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"can't\", 'swim.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokenizer.tokenize(\"I can't swim.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Lemmatization\n",
    "\n",
    "As a definition, lemmatization is to \"**remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.**\"\n",
    "\n",
    "First, we create a raw text and tokenize it into tokens using the function ```word_tokenize```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords ... \n",
    "      is no basis for a system of government. Supreme executive power derives from ... \n",
    "      a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use WordNet lemmatizer for word lemmatization. The WordNet lemmatizer removes affixes only if the resulting word is in its dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', '...', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', '...', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print( [wnl.lemmatize(t) for t in tokens] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "TODO: Write up some motivations of the task.\n",
    "\n",
    "Stemming is to chop off ends of words using some rules. In NLTK, we have several Stemmer to do the job. The following code will try two stemmers in nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ Using PorterStemmer ++\n",
      "\n",
      "['DENNI', ':', 'Listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', '...', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Suprem', 'execut', 'power', 'deriv', 'from', '...', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n",
      "\n",
      "++ Using LancasterStemmer ++\n",
      "\n",
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', '...', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', '...', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print('++ Using PorterStemmer ++\\n')\n",
    "print([porter.stem(t) for t in tokens])\n",
    "print('\\n++ Using LancasterStemmer ++\\n')\n",
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering stop words\n",
    "\n",
    "Stop words are common words that do not contribute to the meaning of a sentence. In general, earch engines and text mining systems filter stop words in the preprocessing step.\n",
    "\n",
    "We can do that by creating a set of English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Can't\", 'contraction']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stops = set(stopwords.words('english'))\n",
    "words = [\"Can't\", 'is', 'a', 'contraction']\n",
    "[word for word in words if word not in english_stops]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-speech tagging\n",
    "\n",
    "Part-of-speech tagging is a process of assigning tags to words in a sentence. In this section, we will instruct you to do tasks as follows.\n",
    "\n",
    "- Explore Tagged Corpora (Brown Corpus)\n",
    "- Perform automatic tagging\n",
    "- Train POS Tagger with (some) basic methods:\n",
    "  * Ngram tagging (unigram, bigram, tagging)\n",
    "  * Transformation-based tagging (Brill tagging)\n",
    "  * Classifier based tagging\n",
    "\n",
    "#### Explore Tagged Corpora (Brown Corpus)\n",
    "\n",
    "Several corpora included with NLTK have been tagged for their part-of-speech. Brown Corpus is an example. If you open a file in the Brown Corpus, you can see tagged sentences like the following example.\n",
    "\n",
    "```The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.```\n",
    "\n",
    "Each token in the sentence is associated with a POS tag.\n",
    "\n",
    "Now we show some tagged words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.brown.tagged_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show tagged words with universal tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'), ('Fulton', 'NOUN'), ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.brown.tagged_words(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further exploration of Brown corpus see the section 2.8, chapter 5 of the book *Natural Language Processing with Python* (Link: [http://www.nltk.org/book/ch05.html](http://www.nltk.org/book/ch05.html))\n",
    "\n",
    "### Perform automatic tagging\n",
    "\n",
    "In ```nltk```, we can perform automatic POS tagging as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = word_tokenize(\"And now for something completely different\")\n",
    "print( nltk.pos_tag(text) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default pos tagger model using in NLTK is maxent_treebank_pos_tagger model (Maxent model trained on the treebank corpus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training POS taggers\n",
    "\n",
    "In this section, we will train POS taggers using Brown corpus. Because of the computational time reason, we select sentences in the category \"news\". You can try to use the whole Brown Corpus.\n",
    "\n",
    "An important thing in NLP projects is evaluating the performance of the trained model on a gold standard test set. Thus, in the first step, we seperate the corpus into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = 4160\n",
      "Test set size = 463\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "brown_sents = brown.sents(categories='news')\n",
    "# You can try to use the whole Brown corpus\n",
    "# brown_tagged_sents = brown.tagged_sents()\n",
    "# brown_sents = brown.sents()\n",
    "\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "print('Training set size = %d' % size)\n",
    "print('Test set size = %d' % (len(brown_sents) - size) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram POS Tagging\n",
    "\n",
    "Unigram taggers are based on a simple statistical algorithm: for each token, assign the tag that is most likely for that particular token. For example, it will assign the tag JJ to any occurrence of the word frequent, since frequent is used as an adjective (e.g. a frequent word) more often than it is used as a verb (e.g. I frequent this cafe).\n",
    "\n",
    "We train an unigram tagging model and evaluate the trained model on the test set using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8129173726701884"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "unigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram POS tagging\n",
    "\n",
    "In the general n-gram taggers, we use the context that contain the current token along with part-of-speech tags of the n-1 preceding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Various', 'JJ'), ('of', 'IN'), ('the', 'AT'), ('apartments', 'NNS'), ('are', 'BER'), ('of', 'IN'), ('the', 'AT'), ('terrace', 'NN'), ('type', 'NN'), (',', ','), ('being', 'BEG'), ('on', 'IN'), ('the', 'AT'), ('ground', 'NN'), ('floor', 'NN'), ('so', 'CS'), ('that', 'CS'), ('entrance', 'NN'), ('is', 'BEZ'), ('direct', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "print(bigram_tagger.tag(brown_sents[2007]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained bigram tagging model to tag unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('population', 'NN'), ('of', 'IN'), ('the', 'AT'), ('Congo', 'NP'), ('is', 'BEZ'), ('13.5', None), ('million', None), (',', None), ('divided', None), ('into', None), ('at', None), ('least', None), ('seven', None), ('major', None), ('``', None), ('culture', None), ('clusters', None), (\"''\", None), ('and', None), ('innumerable', None), ('tribes', None), ('speaking', None), ('400', None), ('separate', None), ('dialects', None), ('.', None)]\n"
     ]
    }
   ],
   "source": [
    "unseen_sent = brown_sents[4203]\n",
    "print( bigram_tagger.tag(unseen_sent) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the bigram tagger is quite percise, but its coverage is low. Thus, the overall accuracy on the test data in very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10196352038273697"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Taggers\n",
    "\n",
    "One way to address the trade-off between accuracy and coverage is to use the more accurate algorithms when we can, but to fall back on algorithms with wider coverage when necessary. For example, we could combine the results of a bigram tagger, a unigram tagger, and a default tagger, as follows:\n",
    "\n",
    "- Try tagging the token with the bigram tagger.\n",
    "- If the bigram tagger is unable to find a tag for the token, try the unigram tagger.\n",
    "- If the unigram tagger is also unable to find a tag, use a default tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8468055417123492"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "t2.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing trained models\n",
    "\n",
    "Training a tagger takes time, so we may want to store a trained model into disk for later re-use. \n",
    "Let's save our tagger t2 to a file t2.pkl. The following code will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "output = open('t2.pkl', 'wb')\n",
    "dump(t2, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the model from the saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "input = open('t2.pkl', 'rb')\n",
    "tagger = load(input)\n",
    "input.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that it can be used for tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('board', 'NN'), (\"'s\", 'NN'), ('action', 'NN'), ('shows', 'NNS'), ('what', 'WDT'), ('free', 'JJ'), ('enterprise', 'NN'), ('is', 'BEZ'), ('up', 'RP'), ('against', 'IN'), ('in', 'IN'), ('our', 'PP$'), ('complex', 'JJ'), ('maze', 'NN'), ('of', 'IN'), ('regulatory', 'NN'), ('laws', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The board's action shows what free enterprise\n",
    "          is up against in our complex maze of regulatory laws .\"\"\"\n",
    "tokens = word_tokenize(text)\n",
    "print( tagger.tag(tokens) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformation-based POS Tagging (Brill Tagging)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned in the course \"Text Mining\", Brill tagging method learn transformation rules from the training data.\n",
    "\n",
    "In order to train a Brill POS tagger, first we need to create an initial tagger. Here we try the unigram tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8129173726701884"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tagger = nltk.UnigramTagger(train_sents)\n",
    "initial_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial tagger, then, is passed to a Brill tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TBL train (fast) (seqs: 4160; tokens: 90521; tpls: 2; min score: 2; min acc: None)\n",
      "Finding initial useful rules...\n",
      "    Found 4397 useful rules.\n",
      "\n",
      "           B      |\n",
      "   S   F   r   O  |        Score = Fixed - Broken\n",
      "   c   i   o   t  |  R     Fixed = num tags changed incorrect -> correct\n",
      "   o   x   k   h  |  u     Broken = num tags changed correct -> incorrect\n",
      "   r   e   e   e  |  l     Other = num tags changed incorrect -> incorrect\n",
      "   e   d   n   r  |  e\n",
      "------------------+-------------------------------------------------------\n",
      " 104 166  62   3  | NN->VB if Pos:TO@[-1]\n",
      "  84 111  27   1  | IN->IN-TL if Pos:NN-TL@[-1] & Word:of@[0]\n",
      "  64  67   3   0  | VBN->VBD if Pos:NP@[-1]\n",
      "  60  60   0   1  | NN->VB if Pos:MD@[-1]\n",
      "  53  53   0   0  | VBD->VBN if Pos:BEDZ@[-1]\n",
      "  45  45   0   2  | VB->NN if Pos:AT@[-1]\n",
      "  45  48   3   0  | VBN->VBD if Pos:PPS@[-1]\n",
      "  42  45   3   0  | PPS->PPO if Pos:VB@[-1] & Word:it@[0]\n",
      "  36  36   0   0  | VBD->VBN if Pos:HVZ@[-1]\n",
      "  35  37   2   0  | CS->DT if Pos:IN@[-1] & Word:that@[0]\n",
      "  34  34   0   0  | VBD->VBN if Pos:BE@[-1]\n",
      "  30  30   0   0  | VB->NN if Pos:JJ@[-1]\n",
      "  28  36   8   1  | TO->IN if Pos:RP@[-1]\n",
      "  27  27   0   0  | VBD->VBN if Pos:HVD@[-1]\n",
      "  24  24   0   0  | VBN->VBD if Pos:WPS@[-1]\n",
      "  19  27   8   0  | TO->IN if Pos:CD@[-1]\n",
      "  19  19   0   0  | VBD->VBN if Pos:HV@[-1]\n",
      "  18  29  11   0  | TO->IN if Pos:IN@[-1]\n",
      "  18  18   0   0  | VBN->VBD if Pos:PPSS@[-1]\n",
      "  17  24   7   0  | PPS->PPO if Pos:IN@[-1] & Word:it@[0]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tbl.template import Template\n",
    "from nltk.tag.brill import Pos, Word\n",
    "from nltk.tag import BrillTaggerTrainer\n",
    "\n",
    "Template._cleartemplates() #clear any templates created in earlier tests\n",
    "templates = [Template(Pos([-1])), Template(Pos([-1]), Word([0]))]\n",
    "tt = BrillTaggerTrainer(initial_tagger, templates, trace=3)\n",
    "brill_tagger = tt.train(train_sents, max_rules=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we evaluate the trained tagger on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8221867836140736"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brill_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier based tagging\n",
    "\n",
    "The ```ClassifierBasedPOSTagger``` uses classification to do part-of-speech tagging. Features are extracted from words, then passed to an internal classifier. The classifier classifies the features and returns a label; in this case, a part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8875710156483604"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "tagger = ClassifierBasedPOSTagger(train=train_sents)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can customize the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import MaxentClassifier\n",
    "# me_tagger = ClassifierBasedPOSTagger(train=train_sents, classifier_builder=MaxentClassifier.train)\n",
    "# me_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom feature detector**\n",
    "\n",
    "In the classifier based tagging, we can customize the feature extraction. Here we use unigram features by defining a function ```unigram_feature_detector```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8094288846805542"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedTagger\n",
    "\n",
    "def unigram_feature_detector(tokens, index, history):\n",
    "    return {'word': tokens[index]}\n",
    "\n",
    "tagger = ClassifierBasedTagger(train=train_sents, feature_detector=unigram_feature_detector)\n",
    "tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information from text\n",
    "\n",
    "The content in the section is a short summary of the [chapter 7, \"Extracting Information from Text\"](http://www.nltk.org/book/ch07.html) of the book *Natural Language Processing with Python*.\n",
    "\n",
    "#### Noun Phrase Chunking\n",
    "\n",
    "In the task, we search for chunks corresponding to individual noun phrases. Base noun phrases do not contain other noun phrases. For example, here is some Wall Street Journal text with NP-chunks marked using brackets:\n",
    "\n",
    "```[ The/DT market/NN ] for/IN \\[ system-management/NN software/NN \\] for/IN \\[ Digital/NNP \\] \\[ 's/POS hardware/NN \\] is/VBZ fragmented/JJ enough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ] should/MD do/VB well/RB there/RB ./.```\n",
    "\n",
    "There are many ways to extract base noun phrases from text. In this section, we will train a tagger from text data, which can extract noun phrases from text. We will learn about tagging problems in the next lecture of the class.\n",
    "\n",
    "We will use CoNLL 2000 Corpus. The corpus contain sentences in which noun phrases are annotated in IOB notation.  As we have seen, each sentence is represented using multiple lines, as shown below:\n",
    "\n",
    "```\n",
    "he PRP B-NP\n",
    "accepted VBD B-VP\n",
    "the DT B-NP\n",
    "position NN I-NP\n",
    "...\n",
    "```\n",
    "\n",
    "In the first step, we load CoNLL 2000 Corpus and print an example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PP Over/IN)\n",
      "  (NP a/DT cup/NN)\n",
      "  (PP of/IN)\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  (VP told/VBD)\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "print(conll2000.chunked_sents('train.txt')[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the CoNLL 2000 corpus contains three chunk types: NP chunks, which we have already seen; VP chunks such as has already delivered; and PP chunks such as because of. Since we are only interested in the NP chunks right now, we can use the chunk_types argument to select them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Over/IN\n",
      "  (NP a/DT cup/NN)\n",
      "  of/IN\n",
      "  (NP coffee/NN)\n",
      "  ,/,\n",
      "  (NP Mr./NNP Stone/NNP)\n",
      "  told/VBD\n",
      "  (NP his/PRP$ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(conll2000.chunked_sents('train.txt', chunk_types=['NP'])[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the training, test data by using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sents = conll2000.chunked_sents('test.txt', chunk_types=['NP']) \n",
    "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a ChunkParser by using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%\n",
      "    Precision:     82.5%\n",
      "    Recall:        86.8%\n",
      "    F-Measure:     84.6%\n"
     ]
    }
   ],
   "source": [
    "class ChunkParser(nltk.ChunkParserI): \n",
    "    def __init__(self, train_sents): \n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)] \n",
    "                      for sent in train_sents] \n",
    "        self.tagger = nltk.TrigramTagger(train_data) \n",
    "    \n",
    "    def parse(self, sentence): \n",
    "        pos_tags = [pos for (word,pos) in sentence] \n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags) \n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags] \n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag) \n",
    "                     in zip(sentence, chunktags)] \n",
    "        return nltk.chunk.conlltags2tree(conlltags)\n",
    "    \n",
    "NPChunker = ChunkParser(train_sents)\n",
    "print(NPChunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the NPChunker to parse a new sentence. First, we need to perform POS tagging on the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('yellow', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sent = 'We saw the yellow dog.'\n",
    "tagged_sent = nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply NPChunker for the POS tagged sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP We/PRP) saw/VBD (NP the/DT yellow/JJ dog/NN) ./.)\n"
     ]
    }
   ],
   "source": [
    "print( NPChunker.parse(tagged_sent) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get two base noun phrases \"We\" and \"the yellow dog\" from the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition\n",
    "\n",
    "Named entities are definite noun phrases that refer to specific types of individuals, such as organizations, persons, dates, and so on. NLTK provides functions to automatically extract named entities from sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S My/PRP$ name/NN is/VBZ (PERSON Donald/NNP Trump/NNP))\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import ne_chunk \n",
    "sent = 'My name is Donald Trump'\n",
    "print( ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relation extraction\n",
    "\n",
    "Relation extraction is to extract relations between entities in the sentence. We often extract relations in triple forms $(X, \\alpha, Y)$, where X and Y are named entities of the required types, and $\\alpha$ is the string of words that intervenes between X and Y. We can use pattern-based methods or machine-learning-based methods. \n",
    "\n",
    "See the section 6 of the [chapter 7](http://www.nltk.org/book/ch07.html) of the book *Natural Language Processing with Python*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing structures of sentences\n",
    "\n",
    "In this section, we will learn how to perform analyzing structures of sentences. We learn to kinds of sentences' structural analysis:\n",
    "\n",
    "- Phrasal structural analysis\n",
    "- Dependency analysis\n",
    "\n",
    "There are so many good tutorials about using nltk. You may want to refer to [chapter 8, Analyzing Sentence Structure](http://www.nltk.org/book/ch08.html) of the book *Natural Language Processing with Python*.\n",
    "\n",
    "Actually, I did not find begin-to-end tools for analyzing structures of sentences. So, for analyzing structures of sentences, I will suggest [Stanford CoreNLP tool](http://stanfordnlp.github.io/CoreNLP/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Bird, Steven; Klein, Ewan; Loper, Edward (2009). *Natural Language Processing with Python*. [http://www.nltk.org/book/](http://www.nltk.org/book/)\n",
    "- [NLTK in 20 minutes](http://www.slideshare.net/japerk/nltk-in-20-minutes), by Jacob Perkins"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
